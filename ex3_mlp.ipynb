{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a13265df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "evaluating loss for context size 9, embedding size 160, hidden_size 200, batch_size 100\n",
      "num params is 297947\n",
      " train loss after 0 steps is 3.205723524093628\n",
      " dev loss after 0 steps is 3.2051217555999756\n",
      " train loss after 10000 steps is 1.9961012601852417\n",
      " dev loss after 10000 steps is 2.078627586364746\n",
      " train loss after 20000 steps is 1.9111264944076538\n",
      " dev loss after 20000 steps is 2.013911485671997\n",
      " train loss after 29999 steps is 1.8949158191680908\n",
      " dev loss after 29999 steps is 2.0109057426452637\n",
      " dev loss after 29999 steps is 2.0106992721557617\n",
      "kedonnah.\n",
      "alyshney.\n",
      "keysha.\n",
      "szona.\n",
      "kathlyne.\n",
      "azalynn.\n",
      "amila.\n",
      "jaydence.\n",
      "orgeniz.\n",
      "pehna.\n",
      "pennee.\n",
      "ricas.\n",
      "bingay.\n",
      "dawno.\n",
      "simphany.\n",
      "lillaamy.\n",
      "drahdi.\n",
      "ivitten.\n",
      "keli.\n",
      "skeniah.\n",
      "train loss is 1.8949158191680908\n",
      "dev loss is 2.0109057426452637\n"
     ]
    }
   ],
   "source": [
    "### start of solution \n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time as time\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"mps\") # training on mac comp, comment if you are too with latest nightly pytorch library\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "\n",
    "def build_dataset(words, device, block_size):\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X, device = device)\n",
    "  Y = torch.tensor(Y, device = device)\n",
    "  return X, Y\n",
    "\n",
    "def eval_loss(context_size, embedding_size, hidden_size, initial_learning_rate, learning_rate_decay_rate,\n",
    "              batch_size, num_steps):\n",
    "    print(context_size)\n",
    "    print(f\"evaluating loss for context size {context_size}, embedding size {embedding_size}, hidden_size {hidden_size}, batch_size {batch_size}\")\n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "    n1 = int(0.8*len(words))\n",
    "    n2 = int(0.9*len(words))\n",
    "\n",
    "    xs_train, ys_train = build_dataset(words[:n1], device, context_size)\n",
    "    xs_dev, ys_dev = build_dataset(words[n1:n2], device, context_size)\n",
    "    xs_eval, ys_eval = build_dataset(words[n2:], device, context_size)          \n",
    "\n",
    "    \n",
    "    W_emb, W1, b1, W2, b2 = get_weights(xs_train, ys_train, context_size, embedding_size, hidden_size, \n",
    "                initial_learning_rate, learning_rate_decay_rate, batch_size, xs_train, ys_train, xs_dev, ys_dev, xs_eval, ys_eval, num_steps)\n",
    "\n",
    "    print(f\"train loss is {get_loss(xs_train, ys_train, W_emb, W1, b1, W2, b2, context_size, embedding_size)}\")\n",
    "    print(f\"dev loss is {get_loss(xs_dev, ys_dev, W_emb, W1, b1, W2, b2, context_size, embedding_size)}\")\n",
    "#     print('number of examples: ', num)\n",
    "#     print(f\"size of train {len(xs_train)} size of dev {len(xs_dev)} size of test {len(xs_eval)}\")\n",
    "    \n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    curr = time.time()\n",
    "\n",
    "def get_loss(x, y, W_emb, W1, b1, W2, b2, context_size, embedding_size):\n",
    "    embs = W_emb[x]\n",
    "#     embs = embs.view(-1, context_size * embedding_size)\n",
    "#     hidden = embs @ W1 + b1\n",
    "#     hidden_tan = (hidden)\n",
    "    embs = embs.view(-1, context_size * embedding_size)\n",
    "    h = torch.tanh(embs @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_weights(xs, ys, context_size, embedding_size, hidden_size, \n",
    "                initial_learning_rate, learning_rate_decay_rate, batch_size, xs_train, ys_train, xs_dev, ys_dev, xs_eval, ys_eval, num_steps):\n",
    "    \n",
    "    # initialize the 'network'\n",
    "    \n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W_emb = torch.normal(0, 1, (27, embedding_size), generator=g, requires_grad=True, device = device)\n",
    "    W1 = torch.normal(0, 0.01, (embedding_size * context_size, hidden_size), generator=g, requires_grad=True, device = device)\n",
    "    b1 = torch.zeros(hidden_size, requires_grad=True, device = device)\n",
    "    W2 = torch.normal(0, 0.01, (hidden_size, 27), generator=g, requires_grad=True, device = device) \n",
    "    b2 = torch.zeros(27, requires_grad=True, device = device) \n",
    "    \n",
    "    params = [W_emb, W1, b1, W2, b2]\n",
    "    num_params = sum([p.nelement() for p in params])\n",
    "    print(f\"num params is {num_params}\")\n",
    "    for k in range(0,num_steps):\n",
    "        ixs = torch.randint(0, xs.shape[0], (batch_size,), generator=g, device=device)\n",
    "        xs_batch = xs[ixs]\n",
    "        ys_batch = ys[ixs]\n",
    "        loss = get_loss(xs_batch, ys_batch, W_emb, W1, b1, W2, b2, context_size, embedding_size)\n",
    "        # backward pass\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "        # update\n",
    "        # learning_rate =  initial_learning_rate * learning_rate_decay_rate ** k #\n",
    "        learning_rate =  0.1 if k < 10000 else 0.01\n",
    "        W_emb.data += -learning_rate * W_emb.grad\n",
    "        W1.data += -learning_rate * W1.grad\n",
    "        b1.data += -learning_rate * b1.grad\n",
    "        W2.data += -learning_rate * W2.grad\n",
    "        b2.data += -learning_rate * b2.grad\n",
    "        if k % 10000 == 0:\n",
    "            print(f\" train loss after {k} steps is {get_loss(xs_train, ys_train, W_emb, W1, b1, W2, b2, context_size, embedding_size).item()}\") \n",
    "            print(f\" dev loss after {k} steps is {get_loss(xs_dev, ys_dev, W_emb, W1, b1, W2, b2, context_size, embedding_size).item()}\") \n",
    "    print(f\" train loss after {k} steps is {get_loss(xs_train, ys_train, W_emb, W1, b1, W2, b2, context_size, embedding_size).item()}\") \n",
    "    print(f\" dev loss after {k} steps is {get_loss(xs_dev, ys_dev, W_emb, W1, b1, W2, b2, context_size, embedding_size).item()}\") \n",
    "    print(f\" dev loss after {k} steps is {get_loss(xs_eval, ys_eval, W_emb, W1, b1, W2, b2, context_size, embedding_size).item()}\") \n",
    "    \n",
    "\n",
    "    for _ in range(20):\n",
    "      out = []\n",
    "      context = [0] * context_size # initialize with all ...\n",
    "      while True:\n",
    "        emb = W_emb[torch.tensor([context], device=device)] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "          break\n",
    "      print(''.join(itos[i] for i in out))\n",
    "\n",
    "    return W_emb, W1, b1, W2, b2\n",
    "\n",
    "eval_loss(9, 160, 200, 0.01, 0.999, 100, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c8574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
